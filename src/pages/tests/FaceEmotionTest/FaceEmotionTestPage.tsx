import { useState, useRef, useEffect, useCallback } from 'react';
import { Camera, Play, Square, RotateCcw, AlertCircle } from 'lucide-react';
import * as faceapi from 'face-api.js';
import TestContainer from '@/components/common/TestContainer/TestContainer';
import Button from '@/components/common/Button/Button';
import Typography from '@/components/common/Typography/Typography';
import AILibraryLoader from '@/utils/aiLibraryLoader';
import {
  StyledTestStep,
  StyledVideoContainer,
  StyledVideo,
  StyledCanvas,
  StyledResultPanel,
  StyledEmotionCard,
  StyledEmotionBar,
  StyledControls,
  StyledLoadingAnimation,
  StyledStatsGrid,
  StyledStatItem,
  StyledErrorMessage,
  StyledSpinner,
  StyledQualitySelect,
  StyledButtonGroup,
  StyledEmotionList,
} from './FaceEmotionTestPage.style';

interface EmotionData {
  name: string;
  value: number;
  color: string;
  emoji: string;
}

interface FaceStats {
  age: number;
  gender: string;
  genderConfidence: number;
  faceCount: number;
}

const FaceEmotionTestPage = () => {
  const [isModelLoaded, setIsModelLoaded] = useState(false);
  const [modelError, setModelError] = useState<string | null>(null);
  const [isStreaming, setIsStreaming] = useState(false);
  const [emotions, setEmotions] = useState<EmotionData[]>([]);
  const [faceStats, setFaceStats] = useState<FaceStats | null>(null);
  const [isDetecting, setIsDetecting] = useState(false);
  const [detectionQuality, setDetectionQuality] = useState<'high' | 'medium' | 'low'>('medium');
  const [loadingStep, setLoadingStep] = useState<string>('TensorFlow.js ë¡œë”© ì¤‘...');

  const videoRef = useRef<HTMLVideoElement>(null);
  const canvasRef = useRef<HTMLCanvasElement>(null);
  const streamRef = useRef<MediaStream | null>(null);
  const detectionIntervalRef = useRef<NodeJS.Timeout | null>(null);
  const isComponentMountedRef = useRef(true);

  // ê°ì •ë³„ ìƒ‰ìƒê³¼ ì´ëª¨ì§€ ì •ì˜
  const emotionConfig = {
    neutral: { color: '#6B7280', emoji: 'ğŸ˜', name: 'í‰ì˜¨í•¨' },
    happy: { color: '#10B981', emoji: 'ğŸ˜Š', name: 'ê¸°ì¨' },
    sad: { color: '#3B82F6', emoji: 'ğŸ˜¢', name: 'ìŠ¬í””' },
    angry: { color: '#EF4444', emoji: 'ğŸ˜ ', name: 'í™”ë‚¨' },
    fearful: { color: '#8B5CF6', emoji: 'ğŸ˜¨', name: 'ë‘ë ¤ì›€' },
    disgusted: { color: '#F59E0B', emoji: 'ğŸ¤¢', name: 'í˜ì˜¤' },
    surprised: { color: '#06B6D4', emoji: 'ğŸ˜²', name: 'ë†€ëŒ' },
  };

  // í’ˆì§ˆ ì„¤ì •
  const qualitySettings = {
    high: { interval: 150, minConfidence: 0.3, inputSize: 416 },
    medium: { interval: 300, minConfidence: 0.5, inputSize: 320 },
    low: { interval: 500, minConfidence: 0.7, inputSize: 224 },
  };

  // ì™„ì „í•œ cleanup í•¨ìˆ˜
  const performCompleteCleanup = useCallback(() => {
    console.log('ğŸ§¹ Performing complete cleanup...');

    // ê°ì§€ ì¤‘ì§€
    if (detectionIntervalRef.current) {
      clearInterval(detectionIntervalRef.current);
      detectionIntervalRef.current = null;
    }

    // ì¹´ë©”ë¼ ìŠ¤íŠ¸ë¦¼ ì™„ì „íˆ ì¤‘ì§€
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => {
        track.stop();
        console.log('ğŸ“· Camera track stopped:', track.kind);
      });
      streamRef.current = null;
    }

    // ë¹„ë””ì˜¤ ìš”ì†Œ ì •ë¦¬
    if (videoRef.current) {
      videoRef.current.srcObject = null;
      videoRef.current.load();
    }

    // ìº”ë²„ìŠ¤ ì •ë¦¬
    if (canvasRef.current) {
      const ctx = canvasRef.current.getContext('2d');
      if (ctx) {
        ctx.clearRect(0, 0, canvasRef.current.width, canvasRef.current.height);
      }
    }

    // ìƒíƒœ ì´ˆê¸°í™” (ì»´í¬ë„ŒíŠ¸ê°€ ë§ˆìš´íŠ¸ëœ ê²½ìš°ì—ë§Œ)
    if (isComponentMountedRef.current) {
      setIsStreaming(false);
      setIsDetecting(false);
      setEmotions([]);
      setFaceStats(null);
    }

    console.log('âœ… Cleanup completed');
  }, []);

  // ê°ì§€ ì¤‘ì§€ í•¨ìˆ˜
  const stopDetection = useCallback(() => {
    console.log('â¹ï¸ Stopping detection...');
    setIsDetecting(false);

    if (detectionIntervalRef.current) {
      clearInterval(detectionIntervalRef.current);
      detectionIntervalRef.current = null;
    }
  }, []);

  // ì¹´ë©”ë¼ ì¤‘ì§€ í•¨ìˆ˜
  const stopCamera = useCallback(() => {
    console.log('ğŸ“· Stopping camera...');
    performCompleteCleanup();
  }, [performCompleteCleanup]);

  // AI ëª¨ë¸ ë¡œë“œ (ESLint ê·œì¹™ ì¤€ìˆ˜)
  useEffect(() => {
    let isCancelled = false;

    const loadTensorFlowAndModels = async (): Promise<void> => {
      try {
        // 1ë‹¨ê³„: TensorFlow.js for Face-API ë¡œë”©
        setLoadingStep('TensorFlow.js ë¡œë”© ì¤‘...');
        const loader = AILibraryLoader.getInstance();
        await loader.loadTensorFlowForFaceAPI(); // ë³€ê²½ë¨

        if (isCancelled || !isComponentMountedRef.current) return;

        // 2ë‹¨ê³„: Backend ì„¤ì •
        setLoadingStep('GPU ë°±ì—”ë“œ ì´ˆê¸°í™” ì¤‘...');
        try {
          await window.tf.setBackend('webgl');
          await window.tf.ready();
          console.log('âœ… WebGL backend initialized');
        } catch (webglError) {
          console.warn('âš ï¸ WebGL failed, falling back to CPU:', webglError);
          await window.tf.setBackend('cpu');
          await window.tf.ready();
          console.log('âœ… CPU backend initialized');
        }

        if (isCancelled || !isComponentMountedRef.current) return;

        // 3ë‹¨ê³„: Face-API ëª¨ë¸ ë¡œë”©
        console.log('ğŸ¤– Loading face-api.js models...');
        const MODEL_URL = '/models';

        setLoadingStep('ì–¼êµ´ ê°ì§€ ëª¨ë¸ ë¡œë”© ì¤‘...');
        await faceapi.nets.ssdMobilenetv1.loadFromUri(MODEL_URL);
        if (isCancelled || !isComponentMountedRef.current) return;

        setLoadingStep('ì–¼êµ´ ëœë“œë§ˆí¬ ëª¨ë¸ ë¡œë”© ì¤‘...');
        await faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL);
        if (isCancelled || !isComponentMountedRef.current) return;

        setLoadingStep('í‘œì • ì¸ì‹ ëª¨ë¸ ë¡œë”© ì¤‘...');
        await faceapi.nets.faceExpressionNet.loadFromUri(MODEL_URL);
        if (isCancelled || !isComponentMountedRef.current) return;

        // ì„ íƒì  ëª¨ë¸
        try {
          setLoadingStep('ë‚˜ì´/ì„±ë³„ ì˜ˆì¸¡ ëª¨ë¸ ë¡œë”© ì¤‘...');
          await faceapi.nets.ageGenderNet.loadFromUri(MODEL_URL);
          console.log('âœ… Age/Gender model loaded');
        } catch (ageGenderError) {
          console.warn('âš ï¸ Age/Gender model failed - continuing without it');
        }

        if (!isCancelled && isComponentMountedRef.current) {
          console.log('ğŸ‰ All models loaded successfully!');
          setIsModelLoaded(true);
          setModelError(null);
          setLoadingStep('ì™„ë£Œ!');
        }
      } catch (error) {
        console.error('âŒ Model loading failed:', error);
        if (!isCancelled && isComponentMountedRef.current) {
          const errorMessage =
            error instanceof Error ? error.message : 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.';
          setModelError(errorMessage);
          setIsModelLoaded(false);
        }
      }
    };

    const timer = setTimeout(() => {
      if (!isCancelled) {
        loadTensorFlowAndModels().catch(error => {
          console.error('Async loading error:', error);
          if (!isCancelled && isComponentMountedRef.current) {
            setModelError('ëª¨ë¸ ë¡œë”© ì¤‘ ì˜ˆê¸°ì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.');
          }
        });
      }
    }, 500);

    return () => {
      isCancelled = true;
      clearTimeout(timer);
    };
  }, []);

  // ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ ì‹œ ì™„ì „í•œ ì •ë¦¬
  useEffect(() => {
    isComponentMountedRef.current = true;

    return () => {
      console.log('ğŸšª Component unmounting - cleaning up all resources...');
      isComponentMountedRef.current = false;
      performCompleteCleanup();
    };
  }, [performCompleteCleanup]);

  // í˜ì´ì§€ ê°€ì‹œì„± ë³€ê²½ ì‹œ ì •ë¦¬
  useEffect(() => {
    const handleVisibilityChange = (): void => {
      if (document.hidden) {
        console.log('ğŸ“± Page hidden - pausing detection...');
        stopDetection();
      }
    };

    const handleBeforeUnload = (): void => {
      console.log('ğŸšª Page unloading - cleaning up...');
      performCompleteCleanup();
    };

    document.addEventListener('visibilitychange', handleVisibilityChange);
    window.addEventListener('beforeunload', handleBeforeUnload);
    window.addEventListener('pagehide', handleBeforeUnload);

    return () => {
      document.removeEventListener('visibilitychange', handleVisibilityChange);
      window.removeEventListener('beforeunload', handleBeforeUnload);
      window.removeEventListener('pagehide', handleBeforeUnload);
    };
  }, [stopDetection, performCompleteCleanup]);

  // ì¹´ë©”ë¼ ì‹œì‘
  const startCamera = useCallback(async (): Promise<void> => {
    if (!isModelLoaded) {
      alert('AI ëª¨ë¸ì´ ì•„ì§ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.');
      return;
    }

    try {
      console.log('ğŸ“· Starting camera...');

      // ê¸°ì¡´ ìŠ¤íŠ¸ë¦¼ì´ ìˆë‹¤ë©´ ì •ë¦¬
      if (streamRef.current) {
        streamRef.current.getTracks().forEach(track => track.stop());
      }

      const mediaStream = await navigator.mediaDevices.getUserMedia({
        video: {
          width: { ideal: 1280, max: 1920 },
          height: { ideal: 720, max: 1080 },
          facingMode: 'user',
          frameRate: { ideal: 30 },
        },
      });

      // ì»´í¬ë„ŒíŠ¸ê°€ ì—¬ì „íˆ ë§ˆìš´íŠ¸ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
      if (!isComponentMountedRef.current) {
        mediaStream.getTracks().forEach(track => track.stop());
        return;
      }

      streamRef.current = mediaStream;

      if (videoRef.current) {
        videoRef.current.srcObject = mediaStream;
        setIsStreaming(true);
        console.log('âœ… Camera started successfully');
      }
    } catch (error) {
      console.error('âŒ Camera error:', error);

      // ì €í’ˆì§ˆë¡œ ì¬ì‹œë„
      try {
        const mediaStream = await navigator.mediaDevices.getUserMedia({
          video: {
            width: { ideal: 640 },
            height: { ideal: 480 },
            facingMode: 'user',
          },
        });

        if (!isComponentMountedRef.current) {
          mediaStream.getTracks().forEach(track => track.stop());
          return;
        }

        streamRef.current = mediaStream;

        if (videoRef.current) {
          videoRef.current.srcObject = mediaStream;
          setIsStreaming(true);
          console.log('âœ… Camera started with lower quality');
        }
      } catch (fallbackError) {
        console.error('âŒ Fallback camera failed:', fallbackError);
        alert('ì¹´ë©”ë¼ ì ‘ê·¼ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ê¶Œí•œì„ í™•ì¸í•´ì£¼ì„¸ìš”.');
      }
    }
  }, [isModelLoaded]);

  // ì–¼êµ´ ê°ì§€ ì‹œì‘
  const startDetection = useCallback((): void => {
    if (
      !videoRef.current ||
      !canvasRef.current ||
      !isModelLoaded ||
      !isComponentMountedRef.current
    ) {
      return;
    }

    console.log('ğŸ¯ Starting face detection...');
    setIsDetecting(true);

    const settings = qualitySettings[detectionQuality];

    const detectFaces = async (): Promise<void> => {
      // ì»´í¬ë„ŒíŠ¸ê°€ ì–¸ë§ˆìš´íŠ¸ë˜ì—ˆê±°ë‚˜ ê°ì§€ê°€ ì¤‘ì§€ë˜ì—ˆìœ¼ë©´ ì¢…ë£Œ
      if (!isComponentMountedRef.current || !detectionIntervalRef.current) {
        return;
      }

      const video = videoRef.current;
      const canvas = canvasRef.current;

      if (!video || !canvas || video.readyState !== 4) return;

      try {
        const videoWidth = video.videoWidth;
        const videoHeight = video.videoHeight;

        if (videoWidth === 0 || videoHeight === 0) return;

        // ìº”ë²„ìŠ¤ í¬ê¸° ì¡°ì •
        if (canvas.width !== videoWidth || canvas.height !== videoHeight) {
          canvas.width = videoWidth;
          canvas.height = videoHeight;
          faceapi.matchDimensions(canvas, { width: videoWidth, height: videoHeight });
        }

        // ì–¼êµ´ ê°ì§€
        const detections = await faceapi
          .detectAllFaces(
            video,
            new faceapi.SsdMobilenetv1Options({
              minConfidence: settings.minConfidence,
              maxResults: 3,
            })
          )
          .withFaceLandmarks()
          .withFaceExpressions();

        // ì»´í¬ë„ŒíŠ¸ ìƒíƒœ ì¬í™•ì¸
        if (!isComponentMountedRef.current) return;

        const ctx = canvas.getContext('2d');
        if (!ctx) return;

        ctx.clearRect(0, 0, canvas.width, canvas.height);

        if (detections.length > 0) {
          const mainDetection = detections.reduce((prev, current) => {
            return prev.detection.box.width * prev.detection.box.height >
              current.detection.box.width * current.detection.box.height
              ? prev
              : current;
          });

          // ìº”ë²„ìŠ¤ ê·¸ë¦¬ê¸° (ê±°ìš¸ íš¨ê³¼)
          ctx.save();
          ctx.scale(-1, 1);
          ctx.translate(-canvas.width, 0);

          const box = mainDetection.detection.box;
          const confidence = Math.round(mainDetection.detection.score * 100);

          // ê²½ê³„ ìƒì
          ctx.strokeStyle = confidence > 70 ? '#10B981' : confidence > 50 ? '#F59E0B' : '#EF4444';
          ctx.lineWidth = 3;
          ctx.strokeRect(box.x, box.y, box.width, box.height);

          // ëœë“œë§ˆí¬
          if (mainDetection.landmarks) {
            ctx.fillStyle = '#06B6D4';
            mainDetection.landmarks.positions.forEach((point: any) => {
              ctx.beginPath();
              ctx.arc(point.x, point.y, 1.5, 0, 2 * Math.PI);
              ctx.fill();
            });
          }

          ctx.restore();

          // í…ìŠ¤íŠ¸ (ì •ìƒ ë°©í–¥)
          ctx.font = 'bold 16px Arial';
          ctx.fillStyle = confidence > 70 ? '#10B981' : confidence > 50 ? '#F59E0B' : '#EF4444';
          ctx.fillText(`ì‹ ë¢°ë„: ${confidence}%`, 10, 30);

          // ê°ì • ë¶„ì„ ë° ìƒíƒœ ì—…ë°ì´íŠ¸
          if (mainDetection.expressions && isComponentMountedRef.current) {
            const expressions = mainDetection.expressions;
            const emotionData: EmotionData[] = Object.entries(expressions).map(([name, value]) => {
              const config = emotionConfig[name as keyof typeof emotionConfig];
              return {
                name: config?.name || name,
                value: Math.round((value as number) * 100),
                color: config?.color || '#6B7280',
                emoji: config?.emoji || 'ğŸ˜',
              };
            });

            emotionData.sort((a, b) => b.value - a.value);
            setEmotions(emotionData);

            if (emotionData[0] && emotionData[0].value > 20) {
              ctx.font = 'bold 20px Arial';
              ctx.fillStyle = emotionData[0].color;
              ctx.fillText(
                `${emotionData[0].emoji} ${emotionData[0].name} ${emotionData[0].value}%`,
                10,
                60
              );
            }
          }
        } else {
          // ì–¼êµ´ ì—†ìŒ
          if (isComponentMountedRef.current) {
            setEmotions([]);
            setFaceStats(null);
          }

          ctx.font = '16px Arial';
          ctx.fillStyle = '#EF4444';
          ctx.fillText('ì–¼êµ´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤', 10, 30);
        }
      } catch (error) {
        console.error('Detection error:', error);
      }
    };

    // interval ì„¤ì •
    detectionIntervalRef.current = setInterval(() => {
      detectFaces().catch(error => {
        console.error('Detection interval error:', error);
      });
    }, settings.interval);
  }, [isModelLoaded, detectionQuality]);

  // ë¹„ë””ì˜¤ ë¡œë“œ ì‹œ ìë™ ê°ì§€ ì‹œì‘
  const handleVideoLoaded = useCallback((): void => {
    if (isStreaming && !isDetecting && isModelLoaded && isComponentMountedRef.current) {
      setTimeout(startDetection, 1000);
    }
  }, [isStreaming, isDetecting, isModelLoaded, startDetection]);

  const shareResult = useCallback((): void => {
    if (emotions.length > 0) {
      const topEmotion = emotions[0];
      const text = `AI ê°ì • ë¶„ì„ ê²°ê³¼: ${topEmotion.emoji} ${topEmotion.name} ${topEmotion.value}%! ì‹¤ì‹œê°„ ì–¼êµ´ ê°ì • ì¸ì‹ í…ŒìŠ¤íŠ¸ë¥¼ ì²´í—˜í•´ë³´ì„¸ìš”.`;

      if (navigator.share) {
        navigator
          .share({
            title: 'AIverse ì‹¤ì‹œê°„ ê°ì • ì¸ì‹ í…ŒìŠ¤íŠ¸',
            text,
            url: window.location.href,
          })
          .catch(error => {
            console.error('Share failed:', error);
          });
      } else {
        navigator.clipboard
          .writeText(`${text} ${window.location.href}`)
          .then(() => {
            alert('ê²°ê³¼ê°€ ë³µì‚¬ë˜ì—ˆìŠµë‹ˆë‹¤!');
          })
          .catch(error => {
            console.error('Copy failed:', error);
          });
      }
    }
  }, [emotions]);

  if (modelError) {
    return (
      <TestContainer
        title="ğŸ­ ì‹¤ì‹œê°„ ì–¼êµ´ ê°ì • ì¸ì‹"
        description="AI ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
      >
        <StyledErrorMessage>
          <AlertCircle size={48} color="#EF4444" />
          <Typography variant="h5" color="#EF4444">
            ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨
          </Typography>
          <Typography variant="body2" color="#6B7280">
            {modelError}
          </Typography>
          <Button variant="primary" onClick={() => window.location.reload()}>
            í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨
          </Button>
        </StyledErrorMessage>
      </TestContainer>
    );
  }

  if (!isModelLoaded) {
    return (
      <TestContainer title="ğŸ­ ì‹¤ì‹œê°„ ì–¼êµ´ ê°ì • ì¸ì‹" description="AI ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤...">
        <StyledLoadingAnimation>
          <StyledSpinner />
          <Typography variant="body1">{loadingStep}</Typography>
          <Typography variant="caption" color="#6B7280">
            ì²˜ìŒ ë°©ë¬¸ ì‹œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œë¡œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤ ğŸ¤–
          </Typography>
        </StyledLoadingAnimation>
      </TestContainer>
    );
  }

  return (
    <TestContainer
      title="ğŸ­ ì‹¤ì‹œê°„ ì–¼êµ´ ê°ì • ì¸ì‹"
      description="AIê°€ ë‹¹ì‹ ì˜ ì–¼êµ´ í‘œì •ì„ ë¶„ì„í•´ì„œ ì‹¤ì‹œê°„ìœ¼ë¡œ ê°ì •ì„ ì¸ì‹í•©ë‹ˆë‹¤!"
      showShare={emotions.length > 0}
      onShare={shareResult}
    >
      <StyledTestStep>
        {/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë¹„ë””ì˜¤ & ìº”ë²„ìŠ¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */}
        <StyledVideoContainer>
          <StyledVideo
            ref={videoRef}
            autoPlay
            muted
            playsInline
            onLoadedData={handleVideoLoaded}
            onCanPlay={handleVideoLoaded}
          />
          <StyledCanvas ref={canvasRef} />
        </StyledVideoContainer>

        {/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì»¨íŠ¸ë¡¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */}
        <StyledControls>
          {!isStreaming ? (
            <Button variant="primary" size="large" onClick={startCamera}>
              <Camera size={20} />
              ì¹´ë©”ë¼ ì‹œì‘
            </Button>
          ) : (
            <StyledButtonGroup>
              {!isDetecting ? (
                <Button variant="primary" onClick={startDetection}>
                  <Play size={16} />
                  ê°ì • ì¸ì‹ ì‹œì‘
                </Button>
              ) : (
                <Button variant="secondary" onClick={stopDetection}>
                  <Square size={16} />
                  ê°ì • ì¸ì‹ ì¤‘ì§€
                </Button>
              )}

              <Button variant="outlined" onClick={stopCamera}>
                <RotateCcw size={16} />
                ì¹´ë©”ë¼ ì¤‘ì§€
              </Button>

              <StyledQualitySelect
                value={detectionQuality}
                onChange={e => setDetectionQuality(e.target.value as any)}
                disabled={isDetecting}
              >
                <option value="high">ê³ í’ˆì§ˆ (ëŠë¦¼)</option>
                <option value="medium">ë³´í†µ (ê¶Œì¥)</option>
                <option value="low">ì €í’ˆì§ˆ (ë¹ ë¦„)</option>
              </StyledQualitySelect>
            </StyledButtonGroup>
          )}
        </StyledControls>

        {/* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê²°ê³¼ íŒ¨ë„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */}
        {(emotions.length > 0 || faceStats) && (
          <StyledResultPanel>
            {faceStats && (
              <StyledStatsGrid>
                <StyledStatItem>
                  <Typography variant="h6" color="#6366F1">
                    {faceStats.age}ì„¸
                  </Typography>
                  <Typography variant="caption" color="#6B7280">
                    ì˜ˆìƒ ë‚˜ì´
                  </Typography>
                </StyledStatItem>
                <StyledStatItem>
                  <Typography variant="h6" color="#8B5CF6">
                    {faceStats.gender === 'male'
                      ? 'ë‚¨ì„±'
                      : faceStats.gender === 'female'
                        ? 'ì—¬ì„±'
                        : 'ì•Œ ìˆ˜ ì—†ìŒ'}
                  </Typography>
                  <Typography variant="caption" color="#6B7280">
                    {faceStats.genderConfidence > 0
                      ? `${faceStats.genderConfidence}% í™•ì‹ `
                      : `ê°ì§€ëœ ì–¼êµ´: ${faceStats.faceCount}ê°œ`}
                  </Typography>
                </StyledStatItem>
              </StyledStatsGrid>
            )}

            {emotions.length > 0 && (
              <>
                <Typography variant="h5" align="center">
                  ì‹¤ì‹œê°„ ê°ì • ë¶„ì„
                </Typography>

                <StyledEmotionList>
                  {emotions.slice(0, 5).map((emotion, index) => (
                    <StyledEmotionCard key={emotion.name} isTop={index === 0}>
                      <div className="emotion-header">
                        <span className="emoji">{emotion.emoji}</span>
                        <Typography variant={index === 0 ? 'h6' : 'body2'}>
                          {emotion.name}
                        </Typography>
                      </div>
                      <StyledEmotionBar color={emotion.color} percentage={emotion.value}>
                        <div className="bar-fill" />
                        <Typography variant="caption">{emotion.value}%</Typography>
                      </StyledEmotionBar>
                    </StyledEmotionCard>
                  ))}
                </StyledEmotionList>
              </>
            )}
          </StyledResultPanel>
        )}

        {isDetecting && (
          <Typography variant="body2" align="center" color="#10B981">
            ğŸ¯ ì‹¤ì‹œê°„ ê°ì • ì¸ì‹ ì¤‘... ì¹´ë©”ë¼ë¥¼ í–¥í•´ ë‹¤ì–‘í•œ í‘œì •ì„ ì§€ì–´ë³´ì„¸ìš”!
          </Typography>
        )}
      </StyledTestStep>
    </TestContainer>
  );
};

export default FaceEmotionTestPage;
